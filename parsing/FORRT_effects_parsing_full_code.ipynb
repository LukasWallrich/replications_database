{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "51fcba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def parse_markdown(markdown_path):\n",
    "    with open(markdown_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    # Remove all HTML tags from content\n",
    "    content = re.sub(r'<[^>]+>', '', content)\n",
    "    parsed_data_fixed = []\n",
    "    field_patterns = {\n",
    "        'Status': r'\\*{0,2}Status\\*{0,2}:\\s*(.*?)\\n',\n",
    "        'Original paper': r'\\*{0,2}Original paper\\(?s?\\)?\\*{0,2}:\\s*(.*?)\\n',\n",
    "        'Critiques': r'\\*{0,2}Critique[s]*\\*{0,2}:\\s*(.*?)\\n',\n",
    "        'Original effect size': r'\\*{0,2}Original effect size[s]*\\*{0,2}:\\s*(.*?)\\n',\n",
    "        'Replication effect size': r'\\*{0,2}Replication effect size[s]*\\*{0,2}:\\s*(.*?)\\n'\n",
    "    }\n",
    "\n",
    "    discipline_sections = re.split(r'\\n###\\s+', content)\n",
    "\n",
    "    for discipline in discipline_sections:\n",
    "        first_line = discipline.split('\\n')[0]\n",
    "        discipline_name = first_line.strip(string.punctuation + string.whitespace)\n",
    "        effect_sections = re.split(r'\\n####\\s*', discipline)[1:]\n",
    "\n",
    "        for effect_section in effect_sections:\n",
    "            lines_in_effect_section = effect_section.split('\\n')\n",
    "            first_line = lines_in_effect_section[0].strip()\n",
    "            if re.match(r\"^###\\s*\\*\\*\", first_line) is not None:\n",
    "                discipline_name = first_line[4:].strip()\n",
    "                continue\n",
    "            effect_name = first_line\n",
    "            description = next((line.strip() for line in lines_in_effect_section[1:] if line.strip()), '')\n",
    "            remaining_effect_lines = '\\n'.join(lines_in_effect_section[1:])\n",
    "            field_data = {field: re.findall(pattern, remaining_effect_lines) for field, pattern in field_patterns.items()}\n",
    "            parsed_data_fixed.append({\n",
    "                'Discipline': discipline_name,\n",
    "                'Effect': effect_name,\n",
    "                'Description': description,\n",
    "                **{field: field_data[field][0] if field_data[field] else '' for field in field_patterns.keys()}\n",
    "            })\n",
    "            \n",
    "    df_fixed = pd.DataFrame(parsed_data_fixed)\n",
    "    df_fixed['Discipline'] = df_fixed['Discipline'].str.strip('*_ ')\n",
    "    df_fixed['Description'] = df_fixed.apply(\n",
    "        lambda row: re.sub(\n",
    "            f\"^{re.escape(row['Effect'])}\",\n",
    "            \"\",\n",
    "            row['Description'].strip(string.punctuation + string.whitespace)\n",
    "        ).strip(string.punctuation + string.whitespace),\n",
    "        axis=1\n",
    "    )\n",
    "    return df_fixed\n",
    "\n",
    "def parse_critiques(parsed_data):\n",
    "\n",
    "    parsed_data = parsed_data[['Discipline', 'Effect', 'Description', 'Critiques']]\n",
    "\n",
    "    # Initialize lists to hold the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Regular expression pattern for extracting critique, link, and notes\n",
    "    pattern = r'\\[\\s*(.+?)\\s*\\]\\((.*?(?=\\)\\s|\\)[.,;]|\\)$))\\)([^[]+?(?=\\[.*\\]\\(|$))'\n",
    "    \n",
    "    #pattern = r'\\[\\s*(.+?)\\s*\\]\\((.+?)\\)\\s*\\[(.*?)\\]'\n",
    "\n",
    "    doi_pattern = r'10\\..*?\\/[^/]*(?=[\\s,;.]\\)|[#?&]|\\/|$)'\n",
    "    n_pattern = r\"\\b[Nn]\\s*=\\s*([\\d\\.]+(?:,[\\d]+)*)(?![\\d\\.])\"\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for idx, row in parsed_data.iterrows():\n",
    "        critiques_str = row['Critiques']\n",
    "\n",
    "        # Needs some complex pre-processing as there are many [] in the notes\n",
    "        # Step 1: Replace \"](\" with a unique marker \"__UNIQUE_MARKER__\"\n",
    "        temp_str = critiques_str.replace(\"](\", \"__UNIQUE_MARKER__\")\n",
    "\n",
    "        # Step 2: Remove all standalone closing square brackets \"]\"\n",
    "        temp_str = re.sub(r'\\](?![^(]*\\()', '', temp_str)\n",
    "\n",
    "        # Step 3: Remove all standalone opening square brackets \"[\" that are not followed by a marked closing square bracket\n",
    "        temp_str = re.sub(r'\\[(?![^\\[\\]]+__UNIQUE_MARKER__)', '', temp_str)\n",
    "\n",
    "        # Step 4: Replace the unique marker back to \"](\"\n",
    "        final_str = temp_str.replace(\"__UNIQUE_MARKER__\", \"](\")\n",
    "\n",
    "        # Step 5: Use regex to extract critique, link, and notes\n",
    "        critiques_list = re.findall(pattern, final_str)\n",
    "\n",
    "\n",
    "        # Create new rows for each critique\n",
    "        for critique, link, notes in critiques_list:\n",
    "            new_row = row.copy()\n",
    "            new_row['Critique'] = critique.strip()\n",
    "            new_row['Link'] = link.strip()\n",
    "            doi_matches = re.findall(doi_pattern, link.strip())\n",
    "            new_row['doi'] = doi_matches[0] if doi_matches else None\n",
    "            # Double decoding - as some dois will be URL encoded\n",
    "            new_row['doi'] = unquote(unquote(new_row['doi'])) if pd.notna(new_row['doi']) else None\n",
    "\n",
    "            # Sort out notes\n",
    "            notes_pattern = r'(.*?)citations?\\s*=\\s*([\\d,]+)\\s*\\(([^)]+)\\)'\n",
    "\n",
    "            match = re.match(notes_pattern, notes.strip())\n",
    "            if match:\n",
    "                new_row['Notes'] = match.group(1).strip().replace(\"*\", \"\").replace(\"_\", \"\")\n",
    "                new_row['Citations'] = match.group(2) + \" (\" + match.group(3) + \")\"\n",
    "            else:\n",
    "                new_row['Notes'] = notes.strip()\n",
    "                new_row['Citations'] = None\n",
    "            \n",
    "            new_row['N critique'] = ' & '.join(re.findall(n_pattern, new_row['Notes']))\n",
    "\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Drop the original 'Critiques' column\n",
    "    new_df.drop(columns=['Critiques'], inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def parse_originals(parsed_data):\n",
    "\n",
    "    parsed_data = parsed_data[['Discipline', 'Effect', 'Description', 'Original paper']]\n",
    "\n",
    "    # Initialize lists to hold the new rows\n",
    "    new_rows = []\n",
    "\n",
    "    # Regular expression pattern for extracting elements of citation\n",
    "    doi_pattern = r'10\\..*?\\/[^/]*(?=[\\s,;.]\\)|[#?&]|\\/|$)'\n",
    "    pattern = r\"\\[([^]]+)\\]\\((.*?(?=\\)\\s|\\)[.,;]|\\)$))\\)(?:[,\\s;]*?([^;]+?[0-9]{4}[a-d]?)(?:[;,. ] ?(.*$))?)?\"\n",
    "\n",
    "    notes_pattern = r'(.*?)citations?(.*?$)'\n",
    "    punctuations_to_remove = ''.join([p for p in string.punctuation if p not in ['~', ')']])\n",
    "    n_pattern = r\"\\b[Nn]\\s*=\\s*([\\d\\.]+(?:,[\\d]+)*)(?![\\d\\.])\"\n",
    "\n",
    "    # = matches[0]\n",
    "\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for idx, row in parsed_data.iterrows():\n",
    "        paper_str = row['Original paper'].replace(\"‘\", \"\").replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "        paper_list = re.findall(pattern, paper_str)\n",
    "\n",
    "        # Check if matching worked\n",
    "        if paper_list:\n",
    "            for title, link, author_year, notes in paper_list:\n",
    "                new_row = row.copy()\n",
    "                new_row['Paper title'] = title.strip()\n",
    "                new_row['Paper ref'] = author_year.strip(string.punctuation + string.whitespace)\n",
    "                new_row['Link'] = link.strip()\n",
    "                doi_matches = re.findall(doi_pattern, link.strip())\n",
    "                new_row['doi'] = doi_matches[0] if doi_matches else None\n",
    "                # Double decoding - as some dois will be URL encoded\n",
    "                new_row['doi'] = unquote(unquote(new_row['doi'])) if pd.notna(new_row['doi']) else None\n",
    "                match = re.match(notes_pattern, notes.strip())\n",
    "                if match:\n",
    "                    new_row['Paper Notes'] = match.group(1).strip().replace(\"*\", \"\").replace(\"_\", \"\").strip(punctuations_to_remove + string.whitespace)\n",
    "                    new_row['Citations'] = match.group(2).strip(punctuations_to_remove + string.whitespace)\n",
    "                else:\n",
    "                    new_row['Paper Notes'] = notes.strip()\n",
    "                    new_row['Citations'] = None\n",
    "                new_row['N orig'] = ' & '.join(re.findall(n_pattern, new_row['Paper Notes']))\n",
    "\n",
    "        else:\n",
    "            # If matching fails\n",
    "            new_row = row.copy()\n",
    "            print(paper_str)\n",
    "            new_row['Paper title'] = None\n",
    "            new_row['Paper ref'] = None\n",
    "            new_row['Link'] = None\n",
    "            new_row['doi'] = None\n",
    "            new_row['Paper Notes'] = paper_str  # Place entire string in 'Paper Notes'\n",
    "\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "\n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Drop the original 'Critiques' column\n",
    "    new_df.drop(columns=['Original paper'], inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def parse_orig_effects(parsed_data):\n",
    "   parsed_data = parsed_data[['Discipline', 'Effect', 'Description', 'Original effect size']].copy()\n",
    "\n",
    "   parsed_data['Original effect size'] = parsed_data['Original effect size'].str.replace(\"_\", \"\")\n",
    "\n",
    "   return parsed_data \n",
    "\n",
    "def parse_critique_effects(parsed_data):\n",
    "   parsed_data = parsed_data[['Discipline', 'Effect', 'Description', 'Replication effect size']].copy()\n",
    "\n",
    "   parsed_data['Replication effect size'] = parsed_data['Replication effect size'].replace(\"_\", \"\")\n",
    "\n",
    "   return parsed_data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4839c5",
   "metadata": {},
   "source": [
    "Manual precleaning steps to parse sections and effects more easily\n",
    "- Replace ####\\n by ####\n",
    "- Replace \\n### \\n by \\n### (ensure this also happens before Social Psych at top)\n",
    "- Replace empty headings: ###\\s+\\n by nothing\n",
    "- delete <p tags and subsequent image errors\n",
    "- remove linebreaks within fields, indicated by \\ at the end of a line\n",
    "\n",
    "Some individual fixes to original papers\n",
    "- Easterlin paradox double link\n",
    "- outgroup bias by a chance win or loss double link\n",
    "- Brehm )\n",
    "- Neonate imitation link\n",
    "- Gender effects of political candidates - link\n",
    "- Differential reinforcement of low rates of behaviour (DRL) - original *paper*\n",
    "\n",
    "And to critiques:\n",
    "-  Implicit God prime increases actual risky behaviour - link\n",
    "- Multiple intelligences - links\n",
    "- Stereotype threat on gender differences in political knowledge - added paper link: https://doi.org/10.1017/XPS.2022.35\n",
    "- Gender effects of political candidates - link scope\n",
    "- Desire-state attribution may govern food sharing in Eurasian jays - section label\n",
    "- Status-legitimacy effect - double link\n",
    "- Automatic imitation - link scope\n",
    "- Transposed word effect - stray [.\n",
    "- Lexical precision on lexical competition - wrong link replaced by NA for now\n",
    "- stray duplicated link removed throughout: https://www.tandfonline.com/doi/full/10.1080/02699931.2018.1468732\n",
    "- Left digit bias - duplicated link\n",
    "- Structural brain-behaviour correlations - the association between behavioural activation and white matter integrity - is corrigendum link even visible? Needs fixing\n",
    "\n",
    "Personal cognitive dissonance - fixed line break for original effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ae059af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parsed_markdown = parse_markdown('FORRT effects v3.md')\n",
    "parsed_markdown[['Discipline', 'Effect', 'Description']].to_csv('parsed_effects.csv', index=False)\n",
    "critiques = parse_critiques(parsed_markdown)\n",
    "critiques.to_csv('parsed_critiques.csv', index=False)\n",
    "critiques.to_excel('parsed_critiques.xlsx', index=False)\n",
    "\n",
    "originals = parse_originals(parsed_markdown)\n",
    "originals.to_csv('parsed_originals.csv', index=False)\n",
    "originals.to_excel('parsed_originals.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bc66230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_abstract(doi):\n",
    "    # Perform HTTP request to CrossRef API\n",
    "    url = f\"http://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    # Extract abstract text\n",
    "    content = response.json()\n",
    "    try:\n",
    "        abstract_raw = content['message']['abstract']\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "    # Remove paragraph tags\n",
    "    abstract_clean = re.sub(r'<jats:p>|<\\/jats:p>', '\\n', abstract_raw)\n",
    "    \n",
    "    # Remove other tags\n",
    "    abstract_clean = re.sub(r'<[^>]*>', ' ', abstract_clean)\n",
    "    \n",
    "    return abstract_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd4c4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "effects_original = parse_orig_effects(parsed_markdown)\n",
    "effects_original.to_csv('effects_original.csv', index=False)\n",
    "\n",
    "effects_critiques = parse_critique_effects(parsed_markdown)\n",
    "effects_critiques.to_excel('effects_critiques.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc61ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# Update DataFrame with abstracts and show progress\n",
    "originals['Abstract'] = originals['doi'].apply(lambda x: get_abstract(x) if pd.notnull(x) else None)\n",
    "critiques['Abstract'] = critiques['doi'].apply(lambda x: get_abstract(x) if pd.notnull(x) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "152e6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "critiques.to_excel('parsed_critiques.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "91f90536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns for clarity\n",
    "originals = originals.rename(columns={\n",
    "    'Paper title': 'Original_Paper_title',\n",
    "    'Paper ref': 'Original_Paper_ref',\n",
    "    'Link': 'Original_Link',\n",
    "    'doi': 'Original_doi',\n",
    "    'Paper Notes': 'Original_Paper_Notes',\n",
    "    'Citations': 'Original_Citations',\n",
    "    'N orig': 'Original_N'\n",
    "})\n",
    "\n",
    "critiques = critiques.rename(columns={\n",
    "    'Critique': 'Critique_text',\n",
    "    'Link': 'Critique_Link',\n",
    "    'doi': 'Critique_doi',\n",
    "    'Notes': 'Critique_Notes',\n",
    "    'Citations': 'Critique_Citations',\n",
    "    'N critique': 'Critique_N'\n",
    "})\n",
    "\n",
    "effects_original = effects_original.rename(columns={\n",
    "    'Original effect size': 'Effect_size_original'\n",
    "})\n",
    "\n",
    "effects_critiques = effects_critiques.rename(columns={\n",
    "    'Replication effect size': 'Effect_size_replication'\n",
    "})\n",
    "\n",
    "# Left joining the dataframes\n",
    "result = originals.merge(critiques, on=['Discipline', 'Effect', 'Description'], how='left') \\\n",
    "                  .merge(effects_original, on=['Discipline', 'Effect', 'Description'], how='left') \\\n",
    "                  .merge(effects_critiques, on=['Discipline', 'Effect', 'Description'], how='left')\n",
    "\n",
    "# Specifying the desired column order\n",
    "column_order = [\n",
    "    'Discipline', 'Effect', 'Description', \n",
    "    'Original_Paper_title', 'Original_Paper_ref', 'Original_Link', 'Original_doi', 'Original_N', 'Effect_size_original', 'Original_Paper_Notes',\n",
    "    'Critique_text', 'Critique_Link', 'Critique_doi', 'Critique_N', 'Effect_size_replication',  'Critique_Notes',\n",
    "    'Original_Citations', 'Critique_Citations'\n",
    "]\n",
    "\n",
    "# Reordering columns in the merged DataFrame\n",
    "result = result[column_order]\n",
    "\n",
    "result.to_excel(\"merged.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6099015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1:\n",
      "  Title: Example 4\n",
      "  URL: https://example.com/abc\n",
      "  Year: ) 2021\n",
      "  Additional Info: \n",
      "Match 2:\n",
      "  Title: Example 5\n",
      "  URL: https://example.com/abc(1,2,3)he\n",
      "  Year: \n",
      "  Additional Info: \n",
      "Match 3:\n",
      "  Title: Example 6\n",
      "  URL: https://example.com/abc(1,2,3)\n",
      "  Year: \n",
      "  Additional Info: \n",
      "Match 4:\n",
      "  Title: Example 7\n",
      "  URL: https://example.com/abc;def\n",
      "  Year: ) 2021\n",
      "  Additional Info: Some Notes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"\\[([^]]+)\\]\\((.*?(?=\\)\\s|\\)[.,;]|\\)$))(?:[,\\s;]*?([^;]+?[0-9]{4}[a-d]?)(?:[;,. ] ?(.*$))?)?\"\n",
    "text = \"\"\"\n",
    "[Example 4](https://example.com/abc) 2021; Additional info\n",
    "[Example 5](https://example.com/abc(1,2,3)he) hello\n",
    "[Example 6](https://example.com/abc(1,2,3)) hello\n",
    "[Example 7](https://example.com/abc;def) 2021, Some Notes\n",
    "\"\"\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "\n",
    "for i, match in enumerate(matches):\n",
    "    print(f\"Match {i+1}:\")\n",
    "    print(f\"  Title: {match[0]}\")\n",
    "    print(f\"  URL: {match[1]}\")\n",
    "    try:\n",
    "        print(f\"  Year: {match[2]}\")\n",
    "        print(f\"  Additional Info: {match[3]}\")\n",
    "    except IndexError:\n",
    "        print(\"  Year, Additional Info: Not found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
